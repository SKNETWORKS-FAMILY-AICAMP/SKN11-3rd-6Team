{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🚀 Phi-2 LoRA Fine-tuning for RunPod A40\n",
        "### Multi-QA Dataset Training with Optimizations\n",
        "\n",
        "**Features:**\n",
        "- ✅ RunPod A40 최적화 설정\n",
        "- ✅ 여러 QA 파일 자동 병합\n",
        "- ✅ 메모리 효율적인 4bit 양자화\n",
        "- ✅ A40 GPU에 맞춘 배치 크기\n",
        "- ✅ WandB 통합 모니터링\n",
        "- ✅ 자동 모델 저장 및 업로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# 🔧 1. 환경 설정 및 라이브러리 설치 (RunPod A40 최적화)\n",
        "# ================================================================\n",
        "\n",
        "# 최신 PyTorch CUDA 12.1 설치 (A40 최적화)\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install transformers datasets peft accelerate bitsandbytes\n",
        "!pip install wandb trl xformers flash-attn --no-build-isolation\n",
        "!pip install --upgrade huggingface_hub\n",
        "\n",
        "# 환경 변수 설정 (A40 최적화)\n",
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "# A40에서 메모리 효율성을 위한 설정\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
        "\n",
        "print(\"✅ 라이브러리 설치 완료!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# 📊 2. GPU 환경 확인 및 최적화 설정\n",
        "# ================================================================\n",
        "\n",
        "import random\n",
        "import torch\n",
        "import json\n",
        "import pandas as pd\n",
        "import gc\n",
        "from datasets import Dataset, concatenate_datasets\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# GPU 정보 상세 확인\n",
        "print(f\"🚀 CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
        "print(f\"🔢 GPU 개수: {torch.cuda.device_count()}\")\n",
        "if torch.cuda.is_available():\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        props = torch.cuda.get_device_properties(i)\n",
        "        print(f\"📱 GPU {i}: {props.name}\")\n",
        "        print(f\"💾 메모리: {props.total_memory / 1024**3:.1f} GB\")\n",
        "        print(f\"🔧 Compute Capability: {props.major}.{props.minor}\")\n",
        "\n",
        "# PyTorch 버전 확인\n",
        "print(f\"⚡ PyTorch 버전: {torch.__version__}\")\n",
        "print(f\"🎯 CUDA 버전: {torch.version.cuda}\")\n",
        "\n",
        "# A40에 최적화된 설정\n",
        "DEVICE_NAME = torch.cuda.get_device_name() if torch.cuda.is_available() else \"CPU\"\n",
        "IS_A40 = \"A40\" in DEVICE_NAME\n",
        "IS_V100 = \"V100\" in DEVICE_NAME\n",
        "IS_A100 = \"A100\" in DEVICE_NAME\n",
        "\n",
        "print(f\"🎮 감지된 GPU: {DEVICE_NAME}\")\n",
        "print(f\"🔍 A40 최적화 모드: {IS_A40}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# 🔐 3. Hugging Face 로그인 및 WandB 설정\n",
        "# ================================================================\n",
        "\n",
        "from huggingface_hub import login\n",
        "import wandb\n",
        "import getpass\n",
        "\n",
        "# Hugging Face 토큰 입력\n",
        "print(\"🔑 Hugging Face 토큰을 입력하세요:\")\n",
        "print(\"토큰 생성: https://huggingface.co/settings/tokens\")\n",
        "hf_token = getpass.getpass(\"HF 토큰: \")\n",
        "login(token=hf_token)\n",
        "print(\"✅ Hugging Face 로그인 완료!\")\n",
        "\n",
        "# WandB 설정 (선택사항)\n",
        "use_wandb = input(\"WandB 사용하시겠습니까? (y/n): \").lower() == 'y'\n",
        "if use_wandb:\n",
        "    wandb_token = getpass.getpass(\"WandB API 키: \")\n",
        "    wandb.login(key=wandb_token)\n",
        "    print(\"✅ WandB 로그인 완료!\")\n",
        "else:\n",
        "    print(\"⏭️ WandB 스킵\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# 🤖 4. 모델 및 토크나이저 로드 (A40 최적화)\n",
        "# ================================================================\n",
        "\n",
        "model_name = \"microsoft/phi-2\"\n",
        "print(f\"🔄 모델 로딩 중: {model_name}\")\n",
        "\n",
        "# 토크나이저 설정\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# A40 최적화 4bit 양자화 설정 (48GB VRAM 활용)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# A40에서 Flash Attention 사용 (성능 향상)\n",
        "try:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        attn_implementation=\"flash_attention_2\",\n",
        "    )\n",
        "    print(\"⚡ Flash Attention 2 활성화됨\")\n",
        "except:\n",
        "    print(\"⚠️ Flash Attention 2 실패, 기본 attention 사용\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "        torch_dtype=torch.bfloat16\n",
        "    )\n",
        "\n",
        "# kbit 훈련용 준비\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# 메모리 정리\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(\"✅ 모델 로딩 완료!\")\n",
        "print(f\"💾 현재 VRAM 사용량: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# ⚙️ 5. LoRA 설정 (A40 최적화)\n",
        "# ================================================================\n",
        "\n",
        "# A40 48GB 메모리를 활용한 더 큰 LoRA 설정\n",
        "lora_config = LoraConfig(\n",
        "    r=32 if IS_A40 else 16,\n",
        "    lora_alpha=64 if IS_A40 else 32,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\", \n",
        "        \"v_proj\",\n",
        "        \"dense\",\n",
        "        \"fc1\",\n",
        "        \"fc2\"\n",
        "    ],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "# LoRA 어댑터 추가\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "print(f\"⚡ LoRA rank: {lora_config.r}\")\n",
        "print(f\"🎯 Target modules: {len(lora_config.target_modules)}개\")\n",
        "print(f\"💾 LoRA 후 VRAM: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "\n",
        "print(\"\\n✅ Part 1 완료! 이제 test_part2.ipynb를 실행하세요.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# 📁 6. 여러 QA 데이터셋 로드 및 병합\n",
        "# ================================================================\n",
        "\n",
        "qa_data = json.load(open('qa_pairs.json', 'r', encoding='utf-8'))\n",
        "# 데이터 셔플\n",
        "random.seed(42)\n",
        "random.shuffle(qa_data)\n",
        "print(f\"🔀 데이터 셔플 완료\")\n",
        "\n",
        "def format_qa_pair(example):\n",
        "    \"\"\"QA 쌍을 훈련용 텍스트로 포맷팅\"\"\"\n",
        "    question = example['question']\n",
        "    answer = example['answer']\n",
        "    context = example['context']\n",
        "    # Context를 포함한 Phi-2에 적합한 프롬프트 템플릿\n",
        "    formatted_text = f\"Context: {context}\\n\\nQuestion: {question}\\nAnswer: {answer}<|endoftext|>\"\n",
        "\n",
        "    return {\"text\": formatted_text}\n",
        "\n",
        "# 데이터셋 생성\n",
        "dataset = Dataset.from_list(qa_data)\n",
        "dataset = dataset.map(format_qa_pair)\n",
        "\n",
        "# 샘플 데이터 확인\n",
        "print(f\"\\n📝 샘플 데이터:\")\n",
        "print(dataset[999][\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# 🎯 8. 토크나이징 (A40 최적화)\n",
        "# ================================================================\n",
        "\n",
        "def tokenize_function_optimized(examples):\n",
        "    \"\"\"A40 최적화 토크나이징 함수\"\"\"\n",
        "    texts = examples[\"text\"] if isinstance(examples[\"text\"], list) else [examples[\"text\"]]\n",
        "    \n",
        "    # A40 48GB를 활용해 더 긴 시퀀스 지원\n",
        "    max_length = 768 if IS_A40 else 512\n",
        "    \n",
        "    tokenized = tokenizer(\n",
        "        texts,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=None\n",
        "    )\n",
        "    \n",
        "    # labels = input_ids (Causal LM)\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized\n",
        "\n",
        "print(\"🔄 토크나이징 시작...\")\n",
        "print(f\"📏 최대 시퀀스 길이: {768 if IS_A40 else 512}\")\n",
        "\n",
        "# 병렬 토크나이징\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function_optimized,\n",
        "    batched=True,\n",
        "    num_proc=8 if IS_A40 else 4,\n",
        "    remove_columns=[col for col in dataset.column_names if col != \"source\"]\n",
        ")\n",
        "\n",
        "print(\"✅ 토크나이징 완료!\")\n",
        "print(f\"📊 토큰화된 샘플 수: {len(tokenized_dataset)}\")\n",
        "\n",
        "# 토큰 길이 통계\n",
        "token_lengths = [len(item['input_ids']) for item in tokenized_dataset[:min(1000, len(tokenized_dataset))]]\n",
        "print(f\"📏 평균 토큰 길이: {sum(token_lengths)/len(token_lengths):.1f}\")\n",
        "print(f\"📏 최대 토큰 길이: {max(token_lengths)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# ⚙️ 9. 훈련 설정 (A40 최적화)\n",
        "# ================================================================\n",
        "\n",
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "# 데이터 콜레이터\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "\n",
        "# WandB 설정 확인\n",
        "try:\n",
        "    use_wandb = 'use_wandb' in globals() and use_wandb\n",
        "except:\n",
        "    use_wandb = False\n",
        "\n",
        "# 훈련 인자\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./phi2-multi-qa-lora\",\n",
        "    num_train_epochs=3,\n",
        "    \n",
        "    # 배치 크기 (안정성과 성능의 균형)\n",
        "    per_device_train_batch_size=6 if IS_A40 else 4,  # A40에서는 6도 가능\n",
        "    gradient_accumulation_steps=4,  # 총 effective batch = 24 or 16\n",
        "    \n",
        "    # 학습률 (LoRA에 최적화)\n",
        "    learning_rate=8e-5,  # 중간값으로 조정\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.03,  # warmup_steps 제거하고 비율만 사용\n",
        "    \n",
        "    # 로깅 및 저장\n",
        "    logging_steps=25,  # 너무 자주 로깅하면 성능 저하\n",
        "    save_steps=200 if IS_A40 else 500,\n",
        "    save_total_limit=3,\n",
        "    \n",
        "    # 평가 설정 추가 (중요!)\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    \n",
        "    # 최적화 설정\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    max_grad_norm=1.0,  # gradient clipping 추가\n",
        "    \n",
        "    # 메모리 최적화\n",
        "    dataloader_pin_memory=False,  # 4bit 양자화시 False\n",
        "    remove_unused_columns=False,\n",
        "    bf16=True,\n",
        "    gradient_checkpointing=True,\n",
        "    \n",
        "    # 기타 설정\n",
        "    report_to=\"wandb\" if use_wandb else None,\n",
        "    run_name=f\"phi2-multi-qa-{len(all_qa_data)}samples\",\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "print(f\"💾 체크포인트 저장 간격: {training_args.save_steps} 스텝\")\n",
        "print(f\"📊 로깅 간격: {training_args.logging_steps} 스텝\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# 🚀 10. 트레이너 설정 및 훈련 시작\n",
        "# ================================================================\n",
        "import gc\n",
        "\n",
        "# TRL SFTTrainer 사용 (더 안정적)\n",
        "try:\n",
        "    from trl import SFTTrainer\n",
        "    \n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        train_dataset=dataset,\n",
        "        args=training_args,\n",
        "        tokenizer=tokenizer,\n",
        "        packing=False,\n",
        "        dataset_text_field=\"text\"\n",
        "    )\n",
        "    print(\"✅ SFTTrainer 설정 완료\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"⚠️ SFTTrainer 실패: {e}\")\n",
        "    print(\"🔄 기본 Trainer로 전환\")\n",
        "    \n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator\n",
        "    )\n",
        "    print(\"✅ 기본 Trainer 설정 완료\")\n",
        "\n",
        "# 훈련 전 메모리 상태\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(f\"💾 훈련 전 VRAM: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "    total_memory = torch.cuda.get_device_properties(0).total_memory\n",
        "    free_memory = total_memory - torch.cuda.memory_allocated()\n",
        "    print(f\"💾 VRAM 여유: {free_memory / 1024**3:.2f} GB\")\n",
        "\n",
        "# 훈련 시작!\n",
        "print(\"\\n🚀 훈련 시작!\")\n",
        "print(f\"🎯 총 샘플 수: {len(all_qa_data)}\")\n",
        "print(f\"📊 에포크: {training_args.num_train_epochs}\")\n",
        "estimated_time = len(all_qa_data) * training_args.num_train_epochs / (batch_size * grad_acc_steps) * 2 / 60\n",
        "print(f\"⏱️ 예상 소요 시간: {estimated_time:.1f}분\")\n",
        "\n",
        "# 훈련 실행\n",
        "trainer.train()\n",
        "\n",
        "print(\"🎉 훈련 완료!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# 💾 11. 모델 저장 및 업로드\n",
        "# ================================================================\n",
        "\n",
        "# LoRA 어댑터 저장\n",
        "output_dir = \"./phi2-multi-qa-lora-final\"\n",
        "trainer.save_model(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(f\"✅ 모델 저장 완료: {output_dir}\")\n",
        "\n",
        "# 모델 크기 확인\n",
        "import os\n",
        "total_size = 0\n",
        "for dirpath, dirnames, filenames in os.walk(output_dir):\n",
        "    for filename in filenames:\n",
        "        filepath = os.path.join(dirpath, filename)\n",
        "        total_size += os.path.getsize(filepath)\n",
        "\n",
        "print(f\"📁 저장된 모델 크기: {total_size / 1024**2:.1f} MB\")\n",
        "\n",
        "# 훈련 완료 후 메모리 정리\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(f\"💾 최종 VRAM 사용량: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "\n",
        "print(\"\\n🎊 모든 작업이 완료되었습니다!\")\n",
        "print(f\"📂 저장 위치: {os.path.abspath(output_dir)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# 10. 선택사항: 허깅페이스 허브에 업로드\n",
        "# ================================================================\n",
        "\n",
        "# 모델 업로드\n",
        "model.push_to_hub(\"cometlee39/phi2-lora-finetuned\")\n",
        "tokenizer.push_to_hub(\"cometlee39/phi2-lora-finetuned\")\n",
        "\n",
        "print(\"\\n모든 과정이 완료되었습니다!\")\n",
        "print(\"LoRA 어댑터가 './phi2-lora-adapter' 폴더에 저장되었습니다.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
