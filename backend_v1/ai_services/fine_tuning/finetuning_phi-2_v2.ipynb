{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸš€ Phi-2 LoRA Fine-tuning for RunPod A40\n",
        "### Multi-QA Dataset Training with Optimizations\n",
        "\n",
        "**Features:**\n",
        "- âœ… RunPod A40 ìµœì í™” ì„¤ì •\n",
        "- âœ… ì—¬ëŸ¬ QA íŒŒì¼ ìë™ ë³‘í•©\n",
        "- âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ 4bit ì–‘ìí™”\n",
        "- âœ… A40 GPUì— ë§ì¶˜ ë°°ì¹˜ í¬ê¸°\n",
        "- âœ… WandB í†µí•© ëª¨ë‹ˆí„°ë§\n",
        "- âœ… ìë™ ëª¨ë¸ ì €ì¥ ë° ì—…ë¡œë“œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# ğŸ”§ 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (RunPod A40 ìµœì í™”)\n",
        "# ================================================================\n",
        "\n",
        "# ìµœì‹  PyTorch CUDA 12.1 ì„¤ì¹˜ (A40 ìµœì í™”)\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install transformers datasets peft accelerate bitsandbytes\n",
        "!pip install wandb trl xformers flash-attn --no-build-isolation\n",
        "!pip install --upgrade huggingface_hub\n",
        "\n",
        "# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (A40 ìµœì í™”)\n",
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "# A40ì—ì„œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•œ ì„¤ì •\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
        "\n",
        "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# ğŸ“Š 2. GPU í™˜ê²½ í™•ì¸ ë° ìµœì í™” ì„¤ì •\n",
        "# ================================================================\n",
        "\n",
        "import random\n",
        "import torch\n",
        "import json\n",
        "import pandas as pd\n",
        "import gc\n",
        "from datasets import Dataset, concatenate_datasets\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# GPU ì •ë³´ ìƒì„¸ í™•ì¸\n",
        "print(f\"ğŸš€ CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
        "print(f\"ğŸ”¢ GPU ê°œìˆ˜: {torch.cuda.device_count()}\")\n",
        "if torch.cuda.is_available():\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        props = torch.cuda.get_device_properties(i)\n",
        "        print(f\"ğŸ“± GPU {i}: {props.name}\")\n",
        "        print(f\"ğŸ’¾ ë©”ëª¨ë¦¬: {props.total_memory / 1024**3:.1f} GB\")\n",
        "        print(f\"ğŸ”§ Compute Capability: {props.major}.{props.minor}\")\n",
        "\n",
        "# PyTorch ë²„ì „ í™•ì¸\n",
        "print(f\"âš¡ PyTorch ë²„ì „: {torch.__version__}\")\n",
        "print(f\"ğŸ¯ CUDA ë²„ì „: {torch.version.cuda}\")\n",
        "\n",
        "# A40ì— ìµœì í™”ëœ ì„¤ì •\n",
        "DEVICE_NAME = torch.cuda.get_device_name() if torch.cuda.is_available() else \"CPU\"\n",
        "IS_A40 = \"A40\" in DEVICE_NAME\n",
        "IS_V100 = \"V100\" in DEVICE_NAME\n",
        "IS_A100 = \"A100\" in DEVICE_NAME\n",
        "\n",
        "print(f\"ğŸ® ê°ì§€ëœ GPU: {DEVICE_NAME}\")\n",
        "print(f\"ğŸ” A40 ìµœì í™” ëª¨ë“œ: {IS_A40}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# ğŸ” 3. Hugging Face ë¡œê·¸ì¸ ë° WandB ì„¤ì •\n",
        "# ================================================================\n",
        "\n",
        "from huggingface_hub import login\n",
        "import wandb\n",
        "import getpass\n",
        "\n",
        "# Hugging Face í† í° ì…ë ¥\n",
        "print(\"ğŸ”‘ Hugging Face í† í°ì„ ì…ë ¥í•˜ì„¸ìš”:\")\n",
        "print(\"í† í° ìƒì„±: https://huggingface.co/settings/tokens\")\n",
        "hf_token = getpass.getpass(\"HF í† í°: \")\n",
        "login(token=hf_token)\n",
        "print(\"âœ… Hugging Face ë¡œê·¸ì¸ ì™„ë£Œ!\")\n",
        "\n",
        "# WandB ì„¤ì • (ì„ íƒì‚¬í•­)\n",
        "use_wandb = input(\"WandB ì‚¬ìš©í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): \").lower() == 'y'\n",
        "if use_wandb:\n",
        "    wandb_token = getpass.getpass(\"WandB API í‚¤: \")\n",
        "    wandb.login(key=wandb_token)\n",
        "    print(\"âœ… WandB ë¡œê·¸ì¸ ì™„ë£Œ!\")\n",
        "else:\n",
        "    print(\"â­ï¸ WandB ìŠ¤í‚µ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# ğŸ¤– 4. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (A40 ìµœì í™”)\n",
        "# ================================================================\n",
        "\n",
        "model_name = \"microsoft/phi-2\"\n",
        "print(f\"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}\")\n",
        "\n",
        "# í† í¬ë‚˜ì´ì € ì„¤ì •\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# A40 ìµœì í™” 4bit ì–‘ìí™” ì„¤ì • (48GB VRAM í™œìš©)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# A40ì—ì„œ Flash Attention ì‚¬ìš© (ì„±ëŠ¥ í–¥ìƒ)\n",
        "try:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        attn_implementation=\"flash_attention_2\",\n",
        "    )\n",
        "    print(\"âš¡ Flash Attention 2 í™œì„±í™”ë¨\")\n",
        "except:\n",
        "    print(\"âš ï¸ Flash Attention 2 ì‹¤íŒ¨, ê¸°ë³¸ attention ì‚¬ìš©\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "        torch_dtype=torch.bfloat16\n",
        "    )\n",
        "\n",
        "# kbit í›ˆë ¨ìš© ì¤€ë¹„\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# ë©”ëª¨ë¦¬ ì •ë¦¬\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(\"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\")\n",
        "print(f\"ğŸ’¾ í˜„ì¬ VRAM ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# âš™ï¸ 5. LoRA ì„¤ì • (A40 ìµœì í™”)\n",
        "# ================================================================\n",
        "\n",
        "# A40 48GB ë©”ëª¨ë¦¬ë¥¼ í™œìš©í•œ ë” í° LoRA ì„¤ì •\n",
        "lora_config = LoraConfig(\n",
        "    r=32 if IS_A40 else 16,\n",
        "    lora_alpha=64 if IS_A40 else 32,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\", \n",
        "        \"v_proj\",\n",
        "        \"dense\",\n",
        "        \"fc1\",\n",
        "        \"fc2\"\n",
        "    ],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "# LoRA ì–´ëŒ‘í„° ì¶”ê°€\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "print(f\"âš¡ LoRA rank: {lora_config.r}\")\n",
        "print(f\"ğŸ¯ Target modules: {len(lora_config.target_modules)}ê°œ\")\n",
        "print(f\"ğŸ’¾ LoRA í›„ VRAM: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "\n",
        "print(\"\\nâœ… Part 1 ì™„ë£Œ! ì´ì œ test_part2.ipynbë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# ğŸ“ 6. ì—¬ëŸ¬ QA ë°ì´í„°ì…‹ ë¡œë“œ ë° ë³‘í•©\n",
        "# ================================================================\n",
        "\n",
        "qa_data = json.load(open('qa_pairs.json', 'r', encoding='utf-8'))\n",
        "# ë°ì´í„° ì…”í”Œ\n",
        "random.seed(42)\n",
        "random.shuffle(qa_data)\n",
        "print(f\"ğŸ”€ ë°ì´í„° ì…”í”Œ ì™„ë£Œ\")\n",
        "\n",
        "def format_qa_pair(example):\n",
        "    \"\"\"QA ìŒì„ í›ˆë ¨ìš© í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…\"\"\"\n",
        "    question = example['question']\n",
        "    answer = example['answer']\n",
        "    context = example['context']\n",
        "    # Contextë¥¼ í¬í•¨í•œ Phi-2ì— ì í•©í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
        "    formatted_text = f\"Context: {context}\\n\\nQuestion: {question}\\nAnswer: {answer}<|endoftext|>\"\n",
        "\n",
        "    return {\"text\": formatted_text}\n",
        "\n",
        "# ë°ì´í„°ì…‹ ìƒì„±\n",
        "dataset = Dataset.from_list(qa_data)\n",
        "dataset = dataset.map(format_qa_pair)\n",
        "\n",
        "# ìƒ˜í”Œ ë°ì´í„° í™•ì¸\n",
        "print(f\"\\nğŸ“ ìƒ˜í”Œ ë°ì´í„°:\")\n",
        "print(dataset[999][\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# ğŸ¯ 8. í† í¬ë‚˜ì´ì§• (A40 ìµœì í™”)\n",
        "# ================================================================\n",
        "\n",
        "def tokenize_function_optimized(examples):\n",
        "    \"\"\"A40 ìµœì í™” í† í¬ë‚˜ì´ì§• í•¨ìˆ˜\"\"\"\n",
        "    texts = examples[\"text\"] if isinstance(examples[\"text\"], list) else [examples[\"text\"]]\n",
        "    \n",
        "    # A40 48GBë¥¼ í™œìš©í•´ ë” ê¸´ ì‹œí€€ìŠ¤ ì§€ì›\n",
        "    max_length = 768 if IS_A40 else 512\n",
        "    \n",
        "    tokenized = tokenizer(\n",
        "        texts,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=None\n",
        "    )\n",
        "    \n",
        "    # labels = input_ids (Causal LM)\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized\n",
        "\n",
        "print(\"ğŸ”„ í† í¬ë‚˜ì´ì§• ì‹œì‘...\")\n",
        "print(f\"ğŸ“ ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´: {768 if IS_A40 else 512}\")\n",
        "\n",
        "# ë³‘ë ¬ í† í¬ë‚˜ì´ì§•\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function_optimized,\n",
        "    batched=True,\n",
        "    num_proc=8 if IS_A40 else 4,\n",
        "    remove_columns=[col for col in dataset.column_names if col != \"source\"]\n",
        ")\n",
        "\n",
        "print(\"âœ… í† í¬ë‚˜ì´ì§• ì™„ë£Œ!\")\n",
        "print(f\"ğŸ“Š í† í°í™”ëœ ìƒ˜í”Œ ìˆ˜: {len(tokenized_dataset)}\")\n",
        "\n",
        "# í† í° ê¸¸ì´ í†µê³„\n",
        "token_lengths = [len(item['input_ids']) for item in tokenized_dataset[:min(1000, len(tokenized_dataset))]]\n",
        "print(f\"ğŸ“ í‰ê·  í† í° ê¸¸ì´: {sum(token_lengths)/len(token_lengths):.1f}\")\n",
        "print(f\"ğŸ“ ìµœëŒ€ í† í° ê¸¸ì´: {max(token_lengths)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# âš™ï¸ 9. í›ˆë ¨ ì„¤ì • (A40 ìµœì í™”)\n",
        "# ================================================================\n",
        "\n",
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "# ë°ì´í„° ì½œë ˆì´í„°\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "\n",
        "# WandB ì„¤ì • í™•ì¸\n",
        "try:\n",
        "    use_wandb = 'use_wandb' in globals() and use_wandb\n",
        "except:\n",
        "    use_wandb = False\n",
        "\n",
        "# í›ˆë ¨ ì¸ì\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./phi2-multi-qa-lora\",\n",
        "    num_train_epochs=3,\n",
        "    \n",
        "    # ë°°ì¹˜ í¬ê¸° (ì•ˆì •ì„±ê³¼ ì„±ëŠ¥ì˜ ê· í˜•)\n",
        "    per_device_train_batch_size=6 if IS_A40 else 4,  # A40ì—ì„œëŠ” 6ë„ ê°€ëŠ¥\n",
        "    gradient_accumulation_steps=4,  # ì´ effective batch = 24 or 16\n",
        "    \n",
        "    # í•™ìŠµë¥  (LoRAì— ìµœì í™”)\n",
        "    learning_rate=8e-5,  # ì¤‘ê°„ê°’ìœ¼ë¡œ ì¡°ì •\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.03,  # warmup_steps ì œê±°í•˜ê³  ë¹„ìœ¨ë§Œ ì‚¬ìš©\n",
        "    \n",
        "    # ë¡œê¹… ë° ì €ì¥\n",
        "    logging_steps=25,  # ë„ˆë¬´ ìì£¼ ë¡œê¹…í•˜ë©´ ì„±ëŠ¥ ì €í•˜\n",
        "    save_steps=200 if IS_A40 else 500,\n",
        "    save_total_limit=3,\n",
        "    \n",
        "    # í‰ê°€ ì„¤ì • ì¶”ê°€ (ì¤‘ìš”!)\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    \n",
        "    # ìµœì í™” ì„¤ì •\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    max_grad_norm=1.0,  # gradient clipping ì¶”ê°€\n",
        "    \n",
        "    # ë©”ëª¨ë¦¬ ìµœì í™”\n",
        "    dataloader_pin_memory=False,  # 4bit ì–‘ìí™”ì‹œ False\n",
        "    remove_unused_columns=False,\n",
        "    bf16=True,\n",
        "    gradient_checkpointing=True,\n",
        "    \n",
        "    # ê¸°íƒ€ ì„¤ì •\n",
        "    report_to=\"wandb\" if use_wandb else None,\n",
        "    run_name=f\"phi2-multi-qa-{len(all_qa_data)}samples\",\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "print(f\"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê°„ê²©: {training_args.save_steps} ìŠ¤í…\")\n",
        "print(f\"ğŸ“Š ë¡œê¹… ê°„ê²©: {training_args.logging_steps} ìŠ¤í…\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# ğŸš€ 10. íŠ¸ë ˆì´ë„ˆ ì„¤ì • ë° í›ˆë ¨ ì‹œì‘\n",
        "# ================================================================\n",
        "import gc\n",
        "\n",
        "# TRL SFTTrainer ì‚¬ìš© (ë” ì•ˆì •ì )\n",
        "try:\n",
        "    from trl import SFTTrainer\n",
        "    \n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        train_dataset=dataset,\n",
        "        args=training_args,\n",
        "        tokenizer=tokenizer,\n",
        "        packing=False,\n",
        "        dataset_text_field=\"text\"\n",
        "    )\n",
        "    print(\"âœ… SFTTrainer ì„¤ì • ì™„ë£Œ\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ SFTTrainer ì‹¤íŒ¨: {e}\")\n",
        "    print(\"ğŸ”„ ê¸°ë³¸ Trainerë¡œ ì „í™˜\")\n",
        "    \n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator\n",
        "    )\n",
        "    print(\"âœ… ê¸°ë³¸ Trainer ì„¤ì • ì™„ë£Œ\")\n",
        "\n",
        "# í›ˆë ¨ ì „ ë©”ëª¨ë¦¬ ìƒíƒœ\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(f\"ğŸ’¾ í›ˆë ¨ ì „ VRAM: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "    total_memory = torch.cuda.get_device_properties(0).total_memory\n",
        "    free_memory = total_memory - torch.cuda.memory_allocated()\n",
        "    print(f\"ğŸ’¾ VRAM ì—¬ìœ : {free_memory / 1024**3:.2f} GB\")\n",
        "\n",
        "# í›ˆë ¨ ì‹œì‘!\n",
        "print(\"\\nğŸš€ í›ˆë ¨ ì‹œì‘!\")\n",
        "print(f\"ğŸ¯ ì´ ìƒ˜í”Œ ìˆ˜: {len(all_qa_data)}\")\n",
        "print(f\"ğŸ“Š ì—í¬í¬: {training_args.num_train_epochs}\")\n",
        "estimated_time = len(all_qa_data) * training_args.num_train_epochs / (batch_size * grad_acc_steps) * 2 / 60\n",
        "print(f\"â±ï¸ ì˜ˆìƒ ì†Œìš” ì‹œê°„: {estimated_time:.1f}ë¶„\")\n",
        "\n",
        "# í›ˆë ¨ ì‹¤í–‰\n",
        "trainer.train()\n",
        "\n",
        "print(\"ğŸ‰ í›ˆë ¨ ì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# ğŸ’¾ 11. ëª¨ë¸ ì €ì¥ ë° ì—…ë¡œë“œ\n",
        "# ================================================================\n",
        "\n",
        "# LoRA ì–´ëŒ‘í„° ì €ì¥\n",
        "output_dir = \"./phi2-multi-qa-lora-final\"\n",
        "trainer.save_model(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(f\"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {output_dir}\")\n",
        "\n",
        "# ëª¨ë¸ í¬ê¸° í™•ì¸\n",
        "import os\n",
        "total_size = 0\n",
        "for dirpath, dirnames, filenames in os.walk(output_dir):\n",
        "    for filename in filenames:\n",
        "        filepath = os.path.join(dirpath, filename)\n",
        "        total_size += os.path.getsize(filepath)\n",
        "\n",
        "print(f\"ğŸ“ ì €ì¥ëœ ëª¨ë¸ í¬ê¸°: {total_size / 1024**2:.1f} MB\")\n",
        "\n",
        "# í›ˆë ¨ ì™„ë£Œ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(f\"ğŸ’¾ ìµœì¢… VRAM ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "\n",
        "print(\"\\nğŸŠ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
        "print(f\"ğŸ“‚ ì €ì¥ ìœ„ì¹˜: {os.path.abspath(output_dir)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# 10. ì„ íƒì‚¬í•­: í—ˆê¹…í˜ì´ìŠ¤ í—ˆë¸Œì— ì—…ë¡œë“œ\n",
        "# ================================================================\n",
        "\n",
        "# ëª¨ë¸ ì—…ë¡œë“œ\n",
        "model.push_to_hub(\"cometlee39/phi2-lora-finetuned\")\n",
        "tokenizer.push_to_hub(\"cometlee39/phi2-lora-finetuned\")\n",
        "\n",
        "print(\"\\nëª¨ë“  ê³¼ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
        "print(\"LoRA ì–´ëŒ‘í„°ê°€ './phi2-lora-adapter' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
