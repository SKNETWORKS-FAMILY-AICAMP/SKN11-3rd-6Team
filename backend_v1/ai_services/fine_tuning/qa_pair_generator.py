import json
import random
import time
import asyncio
from typing import List, Dict, Tuple
from tqdm import tqdm
from collections import defaultdict
import sys
import os
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
import aiofiles

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from rag import RAG
from llm import LLM

@dataclass
class QAResult:
    question: str
    answer: str = None
    context: str = None
    error: str = None

class QAPairGenerator:
    def __init__(self, concurrency_limit: int = 10, batch_size: int = 50):
        self.rag = RAG()
        self.llm = LLM(model_name="gpt-3.5-turbo")
        self.stats = defaultdict(int)
        self.start_time = None
        self.concurrency_limit = concurrency_limit
        self.batch_size = batch_size
        
        # ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ì‹¤í–‰ ì œí•œ
        self.semaphore = asyncio.Semaphore(concurrency_limit)
        
        # ê²°ê³¼ ìºì‹œ (ê°™ì€ ì§ˆë¬¸ ë°˜ë³µ ë°©ì§€)
        self.cache = {}
        
    async def generate_qa_pairs(self, questions_file: str, output_file: str, max_pairs: int = None):
        """ë³‘ë ¬ ì²˜ë¦¬ë¡œ QA ìŒ ìƒì„±"""
        self.start_time = time.time()
        
        # ì§ˆë¬¸ ë¡œë“œ (ë¹„ë™ê¸°)
        print("ğŸ“– Loading questions...")
        questions = await self._load_questions_async(questions_file)
        
        total_questions = len(questions)
        
        if max_pairs:
            questions = random.sample(questions, min(max_pairs, len(questions)))
            print(f"ğŸ² Randomly selected {len(questions)} questions from {total_questions}")
        
        print(f"ğŸ¯ Target: Generate {len(questions)} QA pairs")
        print(f"âš¡ Concurrency limit: {self.concurrency_limit}")
        print(f"ğŸ“¦ Batch size: {self.batch_size}")
        
        # ë°°ì¹˜ë³„ ë³‘ë ¬ ì²˜ë¦¬
        qa_pairs = []
        
        # ë°°ì¹˜ë¡œ ë‚˜ëˆ„ê¸°
        batches = [questions[i:i + self.batch_size] 
                  for i in range(0, len(questions), self.batch_size)]
        
        print(f"ğŸ”„ Processing {len(batches)} batches...")
        
        # ì „ì²´ ì§„í–‰ë„
        total_pbar = tqdm(total=len(questions), desc="Total Progress")
        
        for batch_idx, batch in enumerate(batches):
            print(f"\nğŸš€ Processing batch {batch_idx + 1}/{len(batches)}")
            
            # ë°°ì¹˜ ë‚´ ë³‘ë ¬ ì²˜ë¦¬
            batch_results = await self._process_batch_parallel(batch)
            
            # ê²°ê³¼ ë¶„ë¥˜
            for result in batch_results:
                if result.error:
                    # ì‹¤íŒ¨í•œ ì§ˆë¬¸ì€ ê°„ë‹¨íˆ ë¡œê¹…ë§Œ
                    self.stats['failed'] += 1
                    print(f"âŒ Failed: {result.error}")
                else:
                    qa_pairs.append({
                        "question": result.question,
                        "answer": result.answer,
                        "context": result.context[:1000] if result.context else ""
                    })
                    self.stats['success'] += 1
                
                total_pbar.update(1)
            
                # ë°°ì¹˜ë³„ ì¤‘ê°„ ì €ì¥
            if (batch_idx + 1) % 5 == 0:  # 5ë°°ì¹˜ë§ˆë‹¤
                await self._save_intermediate_async(qa_pairs, output_file, batch_idx + 1)
                self._print_progress_stats(len(qa_pairs) + self.stats['failed'], len(questions))
        
        total_pbar.close()
        
        # ìµœì¢… ê²°ê³¼ ì €ì¥ ë° í†µê³„
        await self._save_final_results_async(qa_pairs, output_file)
        self._print_final_stats(len(questions), qa_pairs)
        
        return qa_pairs
    
    async def _load_questions_async(self, questions_file: str) -> List[Dict]:
        """ë¹„ë™ê¸°ë¡œ ì§ˆë¬¸ íŒŒì¼ ë¡œë“œ"""
        async with aiofiles.open(questions_file, 'r', encoding='utf-8') as f:
            content = await f.read()
            data = json.loads(content)
            return data['questions']
    
    async def _process_batch_parallel(self, batch: List[Dict]) -> List[QAResult]:
        """ë°°ì¹˜ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬"""
        tasks = [self._process_single_question(q_data) for q_data in batch]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ì˜ˆì™¸ ì²˜ë¦¬
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append(QAResult(
                    question=batch[i]['question'],
                    error=str(result)
                ))
            else:
                processed_results.append(result)
        
        return processed_results
    
    async def _process_single_question(self, q_data: Dict) -> QAResult:
        """ë‹¨ì¼ ì§ˆë¬¸ ì²˜ë¦¬ (ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ì‹¤í–‰ ì œí•œ)"""
        async with self.semaphore:
            try:
                country = q_data['country'].lower()
                topic = q_data['topic']
                question = q_data['question']
                
                if topic == 'immigration':
                    topic = 'immigration_regulations'
                elif topic == 'safety':
                    topic = 'immigration_safety'
                    
                # ìºì‹œ í™•ì¸
                cache_key = f"{country}_{topic}_{hash(question)}"
                if cache_key in self.cache:
                    cached_result = self.cache[cache_key]
                    return QAResult(
                        question=question,
                        answer=cached_result['answer'],
                        context=cached_result['context']
                    )
                
                # RAG ê²€ìƒ‰
                context, references = await self._search_context_async(
                    question, country, topic + "_info"
                )
                
                # LLM ë‹µë³€ ìƒì„±
                answer = await self.llm.generate_with_translation(
                    query=question,
                    context=context,
                    references=references,
                    translate_to_korean=False
                )
                
                # ìºì‹œì— ì €ì¥
                self.cache[cache_key] = {
                    'answer': answer,
                    'context': context
                }
                
                return QAResult(
                    question=question,
                    answer=answer,
                    context=context
                )
                
            except Exception as e:
                return QAResult(
                    question=q_data['question'],
                    error=str(e)
                )
    
    async def _search_context_async(self, question: str, country: str, topic: str) -> Tuple[str, List]:
        """RAG ê²€ìƒ‰ (ë¹„ë™ê¸° ë˜í¼)"""
        # RAG.search_with_translationì´ ë™ê¸°ë¼ë©´ ThreadPoolExecutor ì‚¬ìš©
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor(max_workers=5) as executor:
            future = executor.submit(
                self.rag.search_with_translation,
                query=question,
                country=country,
                doc_type=topic
            )
            return await loop.run_in_executor(None, lambda: future.result())
    
    async def _save_intermediate_async(self, qa_pairs: List[Dict], output_file: str, batch_num: int):
        """ë¹„ë™ê¸° ì¤‘ê°„ ì €ì¥"""
        backup_file = f"{output_file}.backup_batch_{batch_num}"
        async with aiofiles.open(backup_file, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(qa_pairs, ensure_ascii=False, indent=2))
        print(f"ğŸ’¾ Backup saved: batch {batch_num}")
    
    async def _save_final_results_async(self, qa_pairs: List[Dict], output_file: str):
        """ë¹„ë™ê¸° ìµœì¢… ê²°ê³¼ ì €ì¥"""
        async with aiofiles.open(output_file, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(qa_pairs, ensure_ascii=False, indent=2))
    
    def _print_progress_stats(self, current: int, total: int):
        """ì§„í–‰ ìƒí™© í†µê³„"""
        elapsed_time = time.time() - self.start_time
        if current > 0:
            avg_time_per_pair = elapsed_time / current
            estimated_remaining = avg_time_per_pair * (total - current)
            success_rate = (self.stats['success'] / current) * 100
            
            print(f"ğŸ“ˆ Progress: {current}/{total}")
            print(f"âœ… Success rate: {success_rate:.1f}%")
            print(f"â±ï¸ Avg time: {avg_time_per_pair:.2f}s/pair")
            print(f"â° ETA: {estimated_remaining/60:.1f} min")
    
    def _print_final_stats(self, total_questions: int, qa_pairs: List[Dict]):
        """ìµœì¢… í†µê³„"""
        total_time = time.time() - self.start_time
        failed_count = self.stats['failed']
        
        print(f"\n{'='*60}")
        print(f"ğŸ‰ QA Pair Generation Completed!")
        print(f"{'='*60}")
        print(f"ğŸ“Š Final Statistics:")
        print(f"  ğŸ“ Total processed: {total_questions}")
        print(f"  âœ… Generated: {len(qa_pairs)}")
        print(f"  âŒ Failed: {failed_count}")
        print(f"  ğŸ“ˆ Success rate: {(len(qa_pairs)/total_questions)*100:.1f}%")
        print(f"  â±ï¸ Total time: {total_time/60:.1f} minutes")
        print(f"  âš¡ Avg time per pair: {total_time/total_questions:.2f}s")
        print(f"  ğŸš€ Throughput: {total_questions*60/total_time:.1f} pairs/minute")


# ì¶”ê°€ ìµœì í™”: ì»¤ë„¥ì…˜ í’€ë§ì„ ìœ„í•œ LLM í´ë˜ìŠ¤ í™•ì¥
class OptimizedLLM(LLM):
    def __init__(self, model_name: str = "gpt-3.5-turbo", max_connections: int = 10):
        super().__init__(model_name)
        self.max_connections = max_connections
        # OpenAI í´ë¼ì´ì–¸íŠ¸ì˜ ì»¤ë„¥ì…˜ í’€ ì„¤ì • (ì‹¤ì œ êµ¬í˜„ì€ LLM í´ë˜ìŠ¤ì— ë”°ë¼ ë‹¤ë¦„)
    
    async def batch_generate(self, requests: List[Dict]) -> List[str]:
        """ë°°ì¹˜ ìš”ì²­ ì²˜ë¦¬"""
        tasks = [
            self.generate_with_translation(**req) 
            for req in requests
        ]
        return await asyncio.gather(*tasks, return_exceptions=True)


# ì‚¬ìš© ì˜ˆì‹œ
# main.pyì—ì„œ í˜¸ì¶œí•˜ê¸° ìœ„í•œ í¸ì˜ í•¨ìˆ˜ë“¤
def create_qa_generator(concurrency_limit: int = 8, batch_size: int = 100) -> 'QAPairGenerator':
    """QA Generator ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    return QAPairGenerator(concurrency_limit=concurrency_limit, batch_size=batch_size)

async def generate_qa_pairs_from_file(
    questions_file: str, 
    output_file: str, 
    max_pairs: int = None,
    concurrency_limit: int = 8,
    batch_size: int = 100
) -> List[Dict]:
    """íŒŒì¼ì—ì„œ ì§ˆë¬¸ì„ ì½ì–´ QA ìŒ ìƒì„± (main.pyì—ì„œ í˜¸ì¶œìš©)"""
    generator = QAPairGenerator(
        concurrency_limit=concurrency_limit,
        batch_size=batch_size
    )
    
    return await generator.generate_qa_pairs(
        questions_file=questions_file,
        output_file=output_file,
        max_pairs=max_pairs
    )

# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸ìš©
async def main():
    # ê¸°ë³¸ ì„¤ì •
    generator = QAPairGenerator(
        concurrency_limit=8,  # ë™ì‹œ ì‹¤í–‰ ìˆ˜ (API ì œí•œì— ë§ê²Œ ì¡°ì •)
        batch_size=100        # ë°°ì¹˜ í¬ê¸°
    )
    
    qa_pairs = await generator.generate_qa_pairs(
        questions_file="questions.json",
        output_file="outputs/qa_pairs.json",
        max_pairs=1000
    )
    
    print(f"Generated {len(qa_pairs)} QA pairs")

if __name__ == "__main__":
    asyncio.run(main())